# -*- coding: utf-8 -*-
"""Cal_Has_Prisec_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zIZJUhdzZ-bZv70SykbSAiW6v9Paczj-

Импорт данных для обучения
[California Housing Prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices?resource=download)
"""

# !pip install folium
# !pip install scikit-learn
# !pip install pandas
# !pip install numpy
# !pip install matplotlib

"""Импорты:"""

import pandas as pd
import matplotlib.pyplot as plt
from google.colab import drive
import folium
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

import kagglehub
from kagglehub import KaggleDatasetAdapter

file_path = "housing.csv"

df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "camnugent/california-housing-prices",
  file_path)

print("First 5 records:", df.head())

"""---

# ***Чистка***

Почистим датасет.
Начнем с пропусков
"""

# сколько пропусков в каждом столбце
print(df.isnull().sum())

# общее количество пропусков
print("Всего пропусков:", df.isnull().sum().sum())

"""Нужно убрать пропуски total_bedrooms"""

df[df["total_bedrooms"] < 3000]["total_bedrooms"].hist(bins=50)
plt.show()

print("Медиана:", df["total_bedrooms"].median())
df["total_bedrooms"] = df["total_bedrooms"].fillna(df["total_bedrooms"].median())

# сколько пропусков в каждом столбце
print(df.isnull().sum())

"""Построим графики по всем значениям признакам, чтобы проверить их на выбросы."""

df.hist(bins=30, figsize=(15, 10), edgecolor='black')
plt.suptitle("Распределения числовых признаков", fontsize=16)
plt.tight_layout()
plt.show()

"""По диаграммам видно, что total_rooms, total_bedrooms, population, households имеют большие выбросы. Решение: логарифмировать или обрубать концы."""

import numpy as np

cols_to_log = ["total_rooms", "total_bedrooms", "population", "households"]

for col in cols_to_log:
    df[col + "_log"] = np.log1p(df[col])

    fig, axes = plt.subplots(1, 2, figsize=(6, 2))

    axes[0].hist(df[col].dropna(), bins=30, edgecolor="black")
    axes[0].set_title(f"{col} (оригинал)")

    axes[1].hist(df[col + "_log"].dropna(), bins=30, edgecolor="black")
    axes[1].set_title(f"{col}_log")

    plt.tight_layout()
    plt.show()

df = df.drop(cols_to_log, axis=1)

""""ocean_proximity" - признак, который нужно разбить на несколько признаков при помощи OneHotEncoding"""

df = pd.get_dummies(df, columns = ["ocean_proximity"], drop_first = True)

print(df.head())

"""---

# ***Добавление новых признаков на основе существующих***

Основную чистку провели, теперь можно поглядеть какие у нас отношения между признаками.
"""

df["rooms_per_household_log"] = (df["total_rooms_log"] / df["households_log"])
df["bedrooms_per_room_log"] = (df["total_bedrooms_log"] / df["total_rooms_log"])
df["population_per_household_log"] = (df["population_log"] / df["households_log"])

"""**Объяснение новых признаков:**


1.   rooms_per_household — среднее количество комнат на одно домохозяйство.
Показывает «просторность жилья»: чем выше значение, тем больше комнат приходится на одну семью/группу жильцов.
2.   bedrooms_per_room — доля спален среди всех комнат.
Характеризует структуру жилья: если почти все комнаты — спальни, жильё обычно более простое и дешевое; низкая доля указывает на просторные и дорогие дома.
3.  population_per_household — среднее количество людей в одном домохозяйстве.
Отражает плотность проживания: большие значения = скученность и более дешёвое жильё, низкие значения = просторные условия и более высокая цена.

Страшная вещь - работаем с геолокацией.
"""

# 1) Рёбра бинов: 20 ячеек = 21 граница
lat_edges = np.linspace(df["latitude"].min(),  df["latitude"].max(),  101)
lon_edges = np.linspace(df["longitude"].min(), df["longitude"].max(), 101)

# 2) Бинируем координаты в индексы 0..19
df["lat_bin"] = pd.cut(df["latitude"],  bins=lat_edges, labels=False, include_lowest=True)
df["lon_bin"] = pd.cut(df["longitude"], bins=lon_edges, labels=False, include_lowest=True)

# выкинем строки, где не попали в бин (NaN)
df2 = df.dropna(subset=["lat_bin", "lon_bin"]).copy()
df2["lat_bin"] = df2["lat_bin"].astype(int)
df2["lon_bin"] = df2["lon_bin"].astype(int)

# 3) Агрегация по ячейкам
grid = (df2.groupby(["lat_bin", "lon_bin"])["median_house_value"]
          .median()
          .reset_index())

# 4) Цветовая функция (зелёный -> красный)
min_val, max_val = df2["median_house_value"].min(), df2["median_house_value"].max()
def value_to_color(v):
    r = int(255 * (v - min_val) / (max_val - min_val))
    g = 255 - r
    return f'#{r:02x}{g:02x}00'

# 5) Карта и прямоугольники
m = folium.Map(location=[df2["latitude"].mean(), df2["longitude"].mean()], zoom_start=6)

for lat_bin, lon_bin, med in grid.itertuples(index=False):
    lat0 = lat_edges[lat_bin]
    lat1 = lat_edges[lat_bin + 1]
    lon0 = lon_edges[lon_bin]
    lon1 = lon_edges[lon_bin + 1]

    folium.Rectangle(
        bounds=[[lat0, lon0], [lat1, lon1]],
        fill=True, fill_opacity=0.6, color=None,
        fill_color=value_to_color(med),
        popup=f"Median: {med:.0f}"
    ).add_to(m)

m

# оставляем только выбросы
outliers = df[df["population_per_household_log"] > np.log1p(100)] # Apply log to 100 for comparison

# создаём карту (центрируем по средним координатам)
m = folium.Map(
    location=[df["latitude"].mean(), df["longitude"].mean()],
    zoom_start=6,
    tiles="cartodbpositron"
)

# добавляем точки
for _, row in outliers.iterrows():
    folium.CircleMarker(
        location=[row["latitude"], row["longitude"]],
        radius=5,
        color="red",
        fill=True,
        fill_opacity=0.8,
        popup=f"population_per_household_log: {row['population_per_household_log']:.2f}" # Display log value
    ).add_to(m)

m  # в Jupyter отобразится карта

"""### Выводы по тепловой карте цен жилья

- **Побережье дороже всего.** Красные квадраты сосредоточены вокруг Лос-Анджелеса, Сан-Диего и Сан-Франциско — это подтверждает ключевую роль близости к океану и крупным агломерациям.  

- **Центральная Калифорния остаётся дешёвой.** Внутренние районы (Фресно, Бейкерсфилд и др.) окрашены в зелёный: медианные цены жилья здесь заметно ниже.  

- **Резкий градиент «океан → глубина штата».** Стоимость жилья падает буквально в пределах десятков километров от побережья.  

- **Южная Калифорния дороже северной.** Район Лос-Анджелеса выделяется на карте как зона наиболее высоких цен, в то время как север штата (Реддинг, Чико) остаётся доступным.

В целом тенденция понятна - ближе к пляжу - лучше, нет добавлять ещё признаки

Но мне нравится идея того, чтобы прологарифмировать стоимость жилья и сравнить графики
"""

df["median_house_value_log"] = np.log1p(df["median_house_value"])
df["median_house_value_log"]

house_value = df["median_house_value"]
house_value_log = np.log1p(house_value)  # log(1+x)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# обычное распределение
axes[0].hist(house_value, bins=30, edgecolor="black")
axes[0].set_title("median_house_value")

# логарифмированное распределение
axes[1].hist(house_value_log, bins=30, edgecolor="black")
axes[1].set_title("log(median_house_value)")

df = df.drop("median_house_value", axis = 1)

plt.tight_layout()
plt.show()

"""# StandardScaler:"""

from sklearn.preprocessing import StandardScaler
import pandas as pd

X = df.drop("median_house_value_log", axis=1)  # признаки
y = df["median_house_value_log"]               # целевая переменная

scaler = StandardScaler()
X_sscaled = scaler.fit_transform(X)

X_sscaled = pd.DataFrame(X_sscaled, columns=X.columns)

X_sscaled.head()

df.columns

"""# MinMax Scaler"""

num_cols = X.select_dtypes(include=[np.number]).columns.tolist()

# Создаём скейлер и применяем
scaler = MinMaxScaler(feature_range=(0, 1))
X_mmscaled = X.copy()
X_mmscaled[num_cols] = scaler.fit_transform(X[num_cols])

# Проверим результат
print(X_mmscaled[num_cols].agg(['min','mean','max']).round(4).T.head())

# Если нужен итоговый DataFrame с целевой
df_scaled = pd.concat([X_mmscaled, y], axis=1)

"""Заебись, теперь у нас есть версия отстандартскейленная и отминмаксскейленная версия. Но теперь нужно будет сравнить."""

df.info()

import numpy as np

cols_to_vs = df.select_dtypes(include="number").drop("median_house_value_log", axis=1).columns.tolist()
cols_to_vs

for col in cols_to_vs:
    fig, axes = plt.subplots(1, 3, figsize=(10, 2))

    axes[0].hist(df[col].dropna(), bins=100, edgecolor="black")
    axes[0].set_title(f"{col} оригинал")

    axes[1].hist(X_sscaled[col].dropna(), bins=30, edgecolor="black")
    axes[1].set_title(f"{col} S scaled")

    axes[2].hist(X_mmscaled[col].dropna(), bins=30, edgecolor="black")
    axes[2].set_title(f"{col} MM scaled")

    plt.tight_layout()
    plt.show()

X = df.drop(columns=["median_house_value_log"])  # признаки
y = df["median_house_value_log"]                 # целевая переменная

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,       # 20% на тест
    random_state=42,     # фиксируем случайность
    shuffle=True         # перемешиваем (по умолчанию True)
)

print("Train shape:", X_train.shape, y_train.shape)
print("Test shape:", X_test.shape, y_test.shape)